{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.16.1\n",
    "import numpy as np\n",
    "np.__version__\n",
    "old = np.load\n",
    "np.load = lambda *a,**k: old(*a,**k,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.layers import Input , Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding # New!\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Activation, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([len(curr) for curr in x_train if len(curr) > 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "text_to_index = reuters.get_word_index()\n",
    "index_to_text = dict((text_to_index[k], k) for k in text_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (8982, 400)\n",
      "x_test shape: (2246, 400)\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000 # size of vocab\n",
    "input_length = 400 # max length of review (must PAD reviews)\n",
    "\n",
    "batch_size = 32\n",
    "# (5000 one-hot -> 50 embedded features)\n",
    "embedding_dims = 50 # size of embedding layer \n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=input_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=input_length)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_c = to_categorical(y_train)\n",
    "y_test_c = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len( np.unique(y_train) ) , len( np.unique(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import time\n",
    "\n",
    "epochs=1\n",
    "batch_size=64\n",
    "tensorboard = TensorBoard(log_dir='./logs/%d' % time.time())\n",
    "earlystopping = EarlyStopping(patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n",
    "from tensorflow.keras.layers import Flatten, RepeatVector,Permute , Multiply,Lambda \n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "\n",
    "TIME_STEPS = input_length\n",
    "OUTPUT_UNITS = len(set(np.unique(y_train).tolist() + np.unique(y_test).tolist()))\n",
    "OUTPUT_UNITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional LSTM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 400)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 400, 128)          640000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400, 128)          131584    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 909,102\n",
      "Trainable params: 909,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "8982/8982 [==============================] - 107s 12ms/sample - loss: 2.0562 - acc: 0.4582 - val_loss: 1.8394 - val_acc: 0.5352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efeac5bd5f8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(TIME_STEPS,))\n",
    "\n",
    "# Encoder\n",
    "emb = Embedding(max_features, 128)(inputs)\n",
    "encoder_level1 = LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "encoder = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(encoder_level1)\n",
    "\n",
    "# Decoder\n",
    "dense_out = Dense(OUTPUT_UNITS, activation='softmax')(encoder)\n",
    "news_decoder = Model(inputs=inputs, outputs=dense_out)\n",
    "\n",
    "news_decoder.compile(loss='categorical_crossentropy', \n",
    "#                    optimizer='adam',\n",
    "                     optimizer=Adam(learning_rate=0.01),\n",
    "                   metrics=['accuracy'])\n",
    "news_decoder.summary()\n",
    "news_decoder.fit(x_train, y_train_c, \n",
    "               batch_size=batch_size,\n",
    "               epochs=1,\n",
    "               validation_data=(x_test, y_test_c),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/5\n",
      "8982/8982 [==============================] - 36s 4ms/sample - loss: 1.6839 - acc: 0.5714 - val_loss: 1.6364 - val_acc: 0.5859\n",
      "Epoch 2/5\n",
      "8982/8982 [==============================] - 36s 4ms/sample - loss: 1.5043 - acc: 0.6075 - val_loss: 1.4868 - val_acc: 0.6073\n",
      "Epoch 3/5\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 1.3645 - acc: 0.6303 - val_loss: 1.4419 - val_acc: 0.6158\n",
      "Epoch 4/5\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 1.2345 - acc: 0.6751 - val_loss: 1.3012 - val_acc: 0.6679\n",
      "Epoch 5/5\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 1.0785 - acc: 0.7211 - val_loss: 1.1996 - val_acc: 0.6955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7eff004c4e10>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_decoder.fit(x_train, y_train_c, \n",
    "               batch_size=batch_size*3,\n",
    "               epochs=5,\n",
    "               validation_data=(x_test, y_test_c),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 36s 4ms/sample - loss: 0.9734 - acc: 0.7457 - val_loss: 1.1367 - val_acc: 0.7128\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.8858 - acc: 0.7650 - val_loss: 1.1214 - val_acc: 0.7244\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.8039 - acc: 0.7918 - val_loss: 1.1018 - val_acc: 0.7476\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.7118 - acc: 0.8149 - val_loss: 1.0433 - val_acc: 0.7538\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.6503 - acc: 0.8312 - val_loss: 1.0372 - val_acc: 0.7560\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.6008 - acc: 0.8454 - val_loss: 1.0587 - val_acc: 0.7591\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.5523 - acc: 0.8623 - val_loss: 1.0550 - val_acc: 0.7573\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.5081 - acc: 0.8733 - val_loss: 1.0649 - val_acc: 0.7698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7eff004c4160>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_decoder.fit(x_train, y_train_c, \n",
    "               batch_size=batch_size*3,\n",
    "               epochs=10,\n",
    "               validation_data=(x_test, y_test_c),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/20\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.4669 - acc: 0.8813 - val_loss: 1.1036 - val_acc: 0.7636\n",
      "Epoch 2/20\n",
      "8982/8982 [==============================] - 36s 4ms/sample - loss: 0.4234 - acc: 0.8950 - val_loss: 1.1524 - val_acc: 0.7703\n",
      "Epoch 3/20\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.3961 - acc: 0.8997 - val_loss: 1.1432 - val_acc: 0.7689\n",
      "Epoch 4/20\n",
      "8982/8982 [==============================] - 35s 4ms/sample - loss: 0.3811 - acc: 0.9029 - val_loss: 1.1289 - val_acc: 0.7711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efeac5bd630>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_decoder.fit(x_train, y_train_c, \n",
    "               batch_size=batch_size*3,\n",
    "               epochs=20,\n",
    "               validation_data=(x_test, y_test_c),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_decoder.save('reuters_92acc_big.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional LSTM API - v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder (?, 400, 100)\n",
      "Dense (?, 400, 1)\n",
      "flatten (?, 400)\n",
      "softmax (?, 400)\n",
      "repear vector (?, 100, 400)\n",
      "Permute 2 1 (?, 400, 100)\n",
      "Multiply enc,att (?, 400, 100)\n",
      "backend.sum (?, 100)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 400, 128)     640000      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 400, 100)     91600       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 400, 100)     80400       lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 400, 1)       101         lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 400)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 400)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 100, 400)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 400, 100)     0           repeat_vector_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 400, 100)     0           lstm_10[0][0]                    \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 100)          0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 46)           4646        lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 816,747\n",
      "Trainable params: 816,747\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/5\n",
      "8982/8982 [==============================] - 108s 12ms/sample - loss: 2.1318 - acc: 0.4067 - val_loss: 1.8336 - val_acc: 0.5249\n",
      "Epoch 2/5\n",
      "8982/8982 [==============================] - 108s 12ms/sample - loss: 1.6817 - acc: 0.5707 - val_loss: 1.5782 - val_acc: 0.5908\n",
      "Epoch 3/5\n",
      "4864/8982 [===============>..............] - ETA: 45s - loss: 1.4876 - acc: 0.6143"
     ]
    }
   ],
   "source": [
    "# V2\n",
    "TIME_STEPS = input_length\n",
    "inputs = Input(shape=(TIME_STEPS,))\n",
    "\n",
    "# Encoder\n",
    "emb = Embedding(max_features, 128)(inputs)\n",
    "enccode_layer1 = LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "encoder = LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(enccode_layer1)\n",
    "\n",
    "# Self-Attention Layer\n",
    "\n",
    "input_dim = encoder.shape\n",
    "print(\"Encoder\",input_dim)\n",
    "attention = Dense(1, activation='tanh')(encoder)\n",
    "print(\"Dense\", attention.shape)\n",
    "attention = Flatten()(attention)\n",
    "print(\"flatten\",attention.shape)\n",
    "attention = Activation('softmax')(attention)\n",
    "print(\"softmax\", attention.shape)\n",
    "units = 100\n",
    "attention = RepeatVector(units)(attention)\n",
    "print(\"repear vector\",attention.shape)\n",
    "attention = Permute([2, 1])(attention)\n",
    "print(\"Permute 2 1\", attention.shape)\n",
    "attention = Multiply()([encoder, attention])\n",
    "print(\"Multiply enc,att\",attention.shape)\n",
    "# attention = Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))(attention)\n",
    "attention = Lambda(lambda x: backend.sum(x, axis=1, keepdims=False))(attention)\n",
    "#attention = backend.sum(attention, axis=1, keepdims=False)\n",
    "print(\"backend.sum\",attention.shape)\n",
    "# Decoder\n",
    "\"\"\"\n",
    "Previous:\n",
    "Encoder shape\n",
    "(?, 100)\n",
    "\"\"\"\n",
    "dense_out = Dense(OUTPUT_UNITS, activation='softmax')(attention)\n",
    "sentiment_decoder = Model(inputs=inputs, outputs=dense_out)\n",
    "\n",
    "sentiment_decoder.compile(loss='categorical_crossentropy', \n",
    "                     optimizer=Adam(learning_rate=0.01),\n",
    "                   metrics=['accuracy'])\n",
    "sentiment_decoder.summary()\n",
    "sentiment_decoder.fit(x_train, y_train_c, \n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_test, y_test_c),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/5\n",
      "8982/8982 [==============================] - 108s 12ms/sample - loss: 0.9715 - acc: 0.7468 - val_loss: 1.1219 - val_acc: 0.7195\n",
      "Epoch 2/5\n",
      "8982/8982 [==============================] - 108s 12ms/sample - loss: 0.9130 - acc: 0.7607 - val_loss: 1.1249 - val_acc: 0.7235\n",
      "Epoch 3/5\n",
      "8982/8982 [==============================] - 108s 12ms/sample - loss: 0.8425 - acc: 0.7803 - val_loss: 1.1052 - val_acc: 0.7355\n",
      "Epoch 4/5\n",
      "8982/8982 [==============================] - 108s 12ms/sample - loss: 0.7796 - acc: 0.7978 - val_loss: 1.0727 - val_acc: 0.7351\n",
      "Epoch 5/5\n",
      "8982/8982 [==============================] - 108s 12ms/sample - loss: 0.7510 - acc: 0.8058 - val_loss: 1.0839 - val_acc: 0.7413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efb4fa33b00>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs=5\n",
    "sentiment_decoder.fit(x_train, y_train_c, \n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_test, y_test_c),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.save('reuters_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in sentiment_decoder.layers[1:]] \n",
    "# Extracts the outputs of the top 12 layers\n",
    "activation_model = Model(inputs=sentiment_decoder.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(test_reviews)\n",
    "for act,layername in zip(activations,layer_outputs):\n",
    "    print(layername)\n",
    "    print(act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.save('imdb_lstm-v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_decoder.metrics[0]\n",
    "# sentiment_decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 8s 4ms/sample - loss: 1.0839 - acc: 0.7413\n",
      "Test score: 1.0838982986745724\n",
      "Test accuracy: 0.7413179\n"
     ]
    }
   ],
   "source": [
    "score, acc = sentiment_decoder.evaluate(x_test, y_test_c, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as index (top 5000 only)\n",
      " [10, 13, 35, 2226, 51, 10, 83, 2048, 12, 1839, 929, 1023, 13, 167, 5, 410, 3, 19, 1, 271, 13, 93, 50, 2273, 71, 10, 66, 83, 3789, 9, 59, 27, 2, 143, 651, 5, 4433, 12, 2633, 265, 307, 44, 58, 18, 1395, 21, 5, 94, 9, 478, 96, 4271, 1, 17, 13, 304, 2, 52, 52, 160, 137, 64, 9, 419, 9, 6, 3, 728, 209, 18, 11, 44, 138, 1771, 1028, 2, 2664, 12, 9, 57, 1, 1015, 242, 36, 653, 1, 2376, 62, 30, 91, 390, 1104, 134, 1, 1939, 102, 66, 5, 25, 65, 5, 1180, 80, 40, 104, 631, 72, 23, 345, 192, 84, 411, 39, 1073, 442, 2, 1111, 3623, 5, 94, 1, 366, 2, 220, 1063, 8, 213, 1236, 29, 1, 2, 1911, 8, 1061, 200, 27, 1073, 3978, 15, 81, 34, 771, 329, 1, 271, 18, 256, 16, 4943, 366, 2, 4781, 141, 76, 9, 57, 45, 33, 23, 21, 2185, 10, 420, 595, 259, 1093, 358, 4, 2873, 14, 70, 14, 1, 3554, 5, 600, 1, 2, 1976, 1, 401, 2, 181, 73, 282, 331, 28, 4, 58, 511, 408, 13, 41, 257, 1107, 18, 22, 212, 5, 103, 9, 15, 621, 10, 1249, 25, 21, 1495, 43, 1289, 312, 3, 19, 11, 73, 8, 2737, 446, 2, 10, 244, 420, 1375, 96, 3, 481, 890, 281, 57, 148, 143, 151, 192, 5, 27, 24, 449, 14, 512, 14, 929, 1023, 2010, 143, 167, 5, 1274, 9, 171, 1, 4432, 13, 21, 192, 2, 47, 68, 35, 108, 114, 385, 12, 1830, 3, 330, 39, 837, 165, 147, 72, 212, 29, 437, 12, 3, 752, 16, 1, 169, 1017, 174, 2, 8, 2906, 6, 579, 260, 93, 156, 96, 193, 1291, 22, 29, 22, 2723, 2185, 153, 1048, 924, 2, 34, 90, 11, 611, 2, 54, 143, 21, 4, 57, 3, 114, 224, 2185]\n",
      "as words (top 5000 only)\n",
      " i was so excited when i first learned that kevin crazy rich was going to become a film the book was way more appealing than i had first imagined it would be and i'm happy to report that jon screen version has my but expectations not to make it sound too simplistic the movie was beautiful and very very funny go see it yes it is a romantic comedy but this has such intriguing social and cultural that it even the fairly away from taking the cinderella story at its face value while the numerous characters had to have their to fit into just two hours we are given enough great dialogue or slightly evil and visual clues to make the friends and family members in come alive all the and ladies in waiting may be slightly overwhelming for people who haven't read the book but anyone with wacky friends and relatives should get it even if they are not asian i liked film's especially clever use of graphics as well as the smooth to score the and locations the perfect and pretty much everything else one of my favorite lines was about having state but you must to watch it for yourself i honestly have not laughed out loud during a film this much in decades oh and i rather liked chris too a totally hot actor even though i'm old enough to be his mother as soon as crazy rich opens i'm going to catch it again the preview was not enough and there were so many little moments that deserve a second or third look now we must all hope that a sequel with the same talented cast and in charge is coming our way before too long thank you all you fabulous asian actors crew writers and who made this possible and no i'm not of even a little bit asian\n",
      "as index (top 5000 only)\n",
      " [48, 3, 354, 17, 11, 13, 3, 52, 354, 19, 10, 1580, 2356, 8, 1, 435, 11, 17, 1012, 54, 689, 10, 78, 21, 383, 11, 17, 85, 42, 3, 434, 4, 55]\n",
      "as words (top 5000 only)\n",
      " what a boring movie this was a very boring film i fell asleep in the cinema this movie deserves no attention i do not recommend this movie because it's a waste of time\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "# Source: https://www.imdb.com/title/tt3104988/reviews\n",
    "\n",
    "test1 = \"\"\"I was so excited when I first learned that Kevin Kwan's \"Crazy Rich Asians\" was going to become a film! The book was way more appealing than I had first imagined it would be, and I'm happy to report that Jon Chu's screen version has surpassed my hopeful-but-wary expectations. Not to make it sound too simplistic, the movie was beautiful and very, very funny. Go see it!\n",
    "Yes, it is a romantic comedy - but this has such intriguing social and cultural undercurrents that it tempts even the fairly observant watcher away from taking the \"Cinderella\" story at its glitzy face value. While the numerous characters had to have their backstories compressed to fit into just two hours, we are given enough great dialogue, effervescent or slightly evil portrayals, and sumptuous visual clues to make the friends and family members in Singapore come alive.\n",
    "All the aunties, cousins and ladies-in-waiting may be slightly overwhelming for people who haven't read the book, but anyone with wacky friends and pompous relatives should get it, even if they are not Asian. \n",
    "I liked film's especially clever use of graphics, as well as the smooth-to-rocking score, the lush and verdant locations, the perfect designer costuming, and pretty much everything else. One of my favorite lines was about having attended Cal State Fullerton; but you must to watch it for yourself. I honestly have not laughed out loud during a film this much in decades. Oh, and I rather liked Chris Pang, too. A totally hot actor, even though I'm old enough to be his mother.\n",
    "As soon as Crazy Rich Asians officially opens, I'm going to catch it again. The preview was not enough, and there were so many little moments that deserve a second or third look. Now we must all hope that a sequel with the same talented cast and Chu in charge is coming our way before too long. Thank you all, you fabulous Asian actors, crew, writers and backers who made this possible. And no, I'm not of even a little bit Asian ancestry.\n",
    "\"\"\"\n",
    "test2 = \"\"\"what a boring movie. This was a very boring film. I fell asleep in the cinema. This movie deserves no attention! I do not recommend this movie because it's a waste of time.\"\"\"\n",
    "\n",
    "def clean_and_get_sequence(text):\n",
    "    # https://keras.io/preprocessing/text/#text_to_word_sequence\n",
    "    from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "    test_sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\n   ',\n",
    "                                          lower=True, split=' ')\n",
    "    # print('before cleaning:', test_sequence)\n",
    "\n",
    "    # drop words not in vocab\n",
    "    test_sequence_cleaned = [s for s in test_sequence if s in text_to_index]\n",
    "    #print('after cleaning:', len(test_sequence_cleaned))\n",
    "\n",
    "    # words that got dropped\n",
    "    #print('dropped words:', set(test_sequence) - set(test_sequence_cleaned))\n",
    "\n",
    "    # map to indices\n",
    "    test_sequence_index = [text_to_index[s] for s in test_sequence_cleaned]\n",
    "    #print('as index\\n', sequence_index)\n",
    "\n",
    "    # filter out top 5000\n",
    "    test_sequence_index_5000 = [i for i in test_sequence_index if i <= 5000]\n",
    "    print('as index (top 5000 only)\\n', test_sequence_index_5000)\n",
    "\n",
    "    # look at review\n",
    "    test_review = ' '.join([index_to_text[i] for i in test_sequence_index_5000])\n",
    "    print('as words (top 5000 only)\\n', test_review)\n",
    "    \n",
    "    return test_sequence_index_5000\n",
    "\n",
    "test1_index = clean_and_get_sequence(test1)\n",
    "test2_index = clean_and_get_sequence(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "test_reviews shape: (2, 400)\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing import sequence\n",
    "\n",
    "test_reviews = [test1_index, test2_index]\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "test_reviews = sequence.pad_sequences(test_reviews, maxlen=input_length)\n",
    "print('test_reviews shape:', test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    1,    4, 1378, 2025,    9,  697, 4622,  111,    8,\n",
       "          25,  109,   29, 3650,   11,  150,  244,  364,   33,   30,   30,\n",
       "        1398,  333,    6,    2,  159,    9, 1084,  363,   13,    2,   71,\n",
       "           9,    2,   71,  117,    4,  225,   78,  206,   10,    9, 1214,\n",
       "           8,    4,  270,    5,    2,    7,  748,   48,    9,    2,    7,\n",
       "         207, 1451,  966, 1864,  793,   97,  133,  336,    7,    4,  493,\n",
       "          98,  273,  104,  284,   25,   39,  338,   22,  905,  220, 3465,\n",
       "         644,   59,   20,    6,  119,   61,   11,   15,   58,  579,   26,\n",
       "          10,   67,    7,    4,  738,   98,   43,   88,  333,  722,   12,\n",
       "          20,    6,   19,  746,   35,   15,   10,    9, 1214,  855,  129,\n",
       "         783,   21,    4, 2280,  244,  364,   51,   16,  299,  452,   16,\n",
       "         515,    4,   99,   29,    5,    4,  364,  281,   48,   10,    9,\n",
       "        1214,   23,  644,   47,   20,  324,   27,   56,    2,    2,    5,\n",
       "         192,  510,   17,   12], dtype=int32), 3)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_7 to have shape (400,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-d9e0767dba80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tests = [test1, test2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# tests = [x_test[0], x_test[1]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for text, probability in zip(tests, pred_prob):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;31m# generate symbolic tensors).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1060\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_7 to have shape (400,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# tests = [test1, test2]\n",
    "# tests = [x_test[0], x_test[1]]\n",
    "pred_prob = sentiment_decoder.predict([x_test[0]])\n",
    "\n",
    "# for text, probability in zip(tests, pred_prob):\n",
    "#     print(text)\n",
    "#     print('positive', 'probability', probability)\n",
    "#     print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test[0].shape\n",
    "pred = sentiment_decoder.predict(x_test[0:1])\n",
    "np.argmax(pred) , y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test[0].shape\n",
    "pred = sentiment_decoder.predict(x_test[3:4])\n",
    "np.argmax(pred) , y_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 400, 128)     640000      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 400, 100)     91600       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 400, 100)     80400       lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 400, 1)       101         lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 400)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 400)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 100, 400)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 400, 100)     0           repeat_vector_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 400, 100)     0           lstm_10[0][0]                    \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 100)          0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 46)           4646        lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 816,747\n",
      "Trainable params: 816,747\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
