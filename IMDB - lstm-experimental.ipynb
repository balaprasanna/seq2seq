{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the IMDB dataset\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Input , Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "text_to_index = imdb.get_word_index()\n",
    "index_to_text = dict((text_to_index[k], k) for k in text_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding # New!\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Activation, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_features = 5000 # size of vocab\n",
    "input_length = 400 # max length of review (must PAD reviews)\n",
    "\n",
    "batch_size = 32\n",
    "# (5000 one-hot -> 50 embedded features)\n",
    "embedding_dims = 50 # size of embedding layer \n",
    "\n",
    "# Pad the sequences (make them same length for neural net.)\n",
    "# Padding add <PAD> for missing words\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=input_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=input_length)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import time\n",
    "\n",
    "epochs=1\n",
    "batch_size=64\n",
    "tensorboard = TensorBoard(log_dir='./logs/%d' % time.time())\n",
    "earlystopping = EarlyStopping(patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n",
    "from tensorflow.keras.layers import Flatten, RepeatVector,Permute , Multiply,Lambda \n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder (?, 400, 100)\n",
      "Dense (?, 400, 1)\n",
      "flatten (?, 400)\n",
      "softmax (?, 400)\n",
      "repear vector (?, 100, 400)\n",
      "Permute 2 1 (?, 400, 100)\n",
      "Multiply enc,att (?, 400, 100)\n",
      "backend.sum (?, 100)\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_36 (InputLayer)           [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_35 (Embedding)        (None, 400, 128)     640000      input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_70 (LSTM)                  (None, 400, 100)     91600       embedding_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_71 (LSTM)                  (None, 400, 100)     80400       lstm_70[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 400, 1)       101         lstm_71[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 400)          0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 400)          0           flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_15 (RepeatVector) (None, 100, 400)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_13 (Permute)            (None, 400, 100)     0           repeat_vector_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 400, 100)     0           lstm_71[0][0]                    \n",
      "                                                                 permute_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 100)          0           multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 1)            101         lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 812,202\n",
      "Trainable params: 812,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "25000/25000 [==============================] - 372s 15ms/sample - loss: 0.4048 - acc: 0.7997 - val_loss: 0.2997 - val_acc: 0.8737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba7d0a4128>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V2\n",
    "TIME_STEPS = input_length\n",
    "inputs = Input(shape=(TIME_STEPS,))\n",
    "\n",
    "# Encoder\n",
    "emb = Embedding(max_features, 128)(inputs)\n",
    "enccode_layer1 = LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "encoder = LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(enccode_layer1)\n",
    "\n",
    "# Self-Attention Layer\n",
    "\n",
    "input_dim = encoder.shape\n",
    "print(\"Encoder\",input_dim)\n",
    "attention = Dense(1, activation='tanh')(encoder)\n",
    "print(\"Dense\", attention.shape)\n",
    "attention = Flatten()(attention)\n",
    "print(\"flatten\",attention.shape)\n",
    "attention = Activation('softmax')(attention)\n",
    "print(\"softmax\", attention.shape)\n",
    "units = 100\n",
    "attention = RepeatVector(units)(attention)\n",
    "print(\"repear vector\",attention.shape)\n",
    "attention = Permute([2, 1])(attention)\n",
    "print(\"Permute 2 1\", attention.shape)\n",
    "attention = Multiply()([encoder, attention])\n",
    "print(\"Multiply enc,att\",attention.shape)\n",
    "# attention = Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))(attention)\n",
    "attention = Lambda(lambda x: backend.sum(x, axis=1, keepdims=False))(attention)\n",
    "#attention = backend.sum(attention, axis=1, keepdims=False)\n",
    "print(\"backend.sum\",attention.shape)\n",
    "# Decoder\n",
    "\"\"\"\n",
    "Previous:\n",
    "Encoder shape\n",
    "(?, 100)\n",
    "\"\"\"\n",
    "dense_out = Dense(1, activation='sigmoid')(attention)\n",
    "sentiment_decoder = Model(inputs=inputs, outputs=dense_out)\n",
    "\n",
    "sentiment_decoder.compile(loss='binary_crossentropy', \n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "sentiment_decoder.summary()\n",
    "sentiment_decoder.fit(x_train, y_train, \n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_test, y_test),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 368s 15ms/sample - loss: 0.2395 - acc: 0.9029 - val_loss: 0.2720 - val_acc: 0.8853\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 369s 15ms/sample - loss: 0.1953 - acc: 0.9251 - val_loss: 0.2901 - val_acc: 0.8802\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 369s 15ms/sample - loss: 0.1655 - acc: 0.9390 - val_loss: 0.2960 - val_acc: 0.8788\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 369s 15ms/sample - loss: 0.1397 - acc: 0.9496 - val_loss: 0.3309 - val_acc: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba86d2cb70>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs=5\n",
    "sentiment_decoder.fit(x_train, y_train, \n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_test, y_test),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.save(\"imdb_exp1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional LSTM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = input_length\n",
    "inputs = Input(shape=(TIME_STEPS,))\n",
    "\n",
    "# Encoder\n",
    "emb = Embedding(max_features, 128)(inputs)\n",
    "encoder1 = LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb)\n",
    "encoder = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(encoder1)\n",
    "\n",
    "# Self-Attention Layer\n",
    "\n",
    "# att_Dense(??)(encoder)\n",
    "# z = keras.layers.add([x, y])\n",
    "\n",
    "# Decoder\n",
    "dense_out = Dense(1, activation='sigmoid')(encoder)\n",
    "sentiment_decoder = Model(inputs=inputs, outputs=dense_out)\n",
    "\n",
    "sentiment_decoder.compile(loss='binary_crossentropy', \n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])\n",
    "sentiment_decoder.summary()\n",
    "sentiment_decoder.fit(x_train, y_train, \n",
    "               batch_size=batch_size,\n",
    "               epochs=1,\n",
    "               validation_data=(x_test, y_test),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.fit(x_train, y_train, \n",
    "               batch_size=batch_size,\n",
    "               epochs=2,\n",
    "               validation_data=(x_test, y_test),\n",
    "               callbacks=[tensorboard, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.save('imdb_lstm-v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in sentiment_decoder.layers[1:]] \n",
    "# Extracts the outputs of the top 12 layers\n",
    "activation_model = Model(inputs=sentiment_decoder.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(test_reviews)\n",
    "for act,layername in zip(activations,layer_outputs):\n",
    "    print(layername)\n",
    "    print(act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.save('imdb_lstm-v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_decoder.metrics[0]\n",
    "# sentiment_decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 91s 4ms/sample - loss: 0.3309 - acc: 0.8775\n",
      "Test score: 0.3309473884153366\n",
      "Test accuracy: 0.87748\n"
     ]
    }
   ],
   "source": [
    "score, acc = sentiment_decoder.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as index (top 5000 only)\n",
      " [10, 13, 35, 2226, 51, 10, 83, 2048, 12, 1839, 929, 1023, 13, 167, 5, 410, 3, 19, 1, 271, 13, 93, 50, 2273, 71, 10, 66, 83, 3789, 9, 59, 27, 2, 143, 651, 5, 4433, 12, 2633, 265, 307, 44, 58, 18, 1395, 21, 5, 94, 9, 478, 96, 4271, 1, 17, 13, 304, 2, 52, 52, 160, 137, 64, 9, 419, 9, 6, 3, 728, 209, 18, 11, 44, 138, 1771, 1028, 2, 2664, 12, 9, 57, 1, 1015, 242, 36, 653, 1, 2376, 62, 30, 91, 390, 1104, 134, 1, 1939, 102, 66, 5, 25, 65, 5, 1180, 80, 40, 104, 631, 72, 23, 345, 192, 84, 411, 39, 1073, 442, 2, 1111, 3623, 5, 94, 1, 366, 2, 220, 1063, 8, 213, 1236, 29, 1, 2, 1911, 8, 1061, 200, 27, 1073, 3978, 15, 81, 34, 771, 329, 1, 271, 18, 256, 16, 4943, 366, 2, 4781, 141, 76, 9, 57, 45, 33, 23, 21, 2185, 10, 420, 595, 259, 1093, 358, 4, 2873, 14, 70, 14, 1, 3554, 5, 600, 1, 2, 1976, 1, 401, 2, 181, 73, 282, 331, 28, 4, 58, 511, 408, 13, 41, 257, 1107, 18, 22, 212, 5, 103, 9, 15, 621, 10, 1249, 25, 21, 1495, 43, 1289, 312, 3, 19, 11, 73, 8, 2737, 446, 2, 10, 244, 420, 1375, 96, 3, 481, 890, 281, 57, 148, 143, 151, 192, 5, 27, 24, 449, 14, 512, 14, 929, 1023, 2010, 143, 167, 5, 1274, 9, 171, 1, 4432, 13, 21, 192, 2, 47, 68, 35, 108, 114, 385, 12, 1830, 3, 330, 39, 837, 165, 147, 72, 212, 29, 437, 12, 3, 752, 16, 1, 169, 1017, 174, 2, 8, 2906, 6, 579, 260, 93, 156, 96, 193, 1291, 22, 29, 22, 2723, 2185, 153, 1048, 924, 2, 34, 90, 11, 611, 2, 54, 143, 21, 4, 57, 3, 114, 224, 2185]\n",
      "as words (top 5000 only)\n",
      " i was so excited when i first learned that kevin crazy rich was going to become a film the book was way more appealing than i had first imagined it would be and i'm happy to report that jon screen version has my but expectations not to make it sound too simplistic the movie was beautiful and very very funny go see it yes it is a romantic comedy but this has such intriguing social and cultural that it even the fairly away from taking the cinderella story at its face value while the numerous characters had to have their to fit into just two hours we are given enough great dialogue or slightly evil and visual clues to make the friends and family members in come alive all the and ladies in waiting may be slightly overwhelming for people who haven't read the book but anyone with wacky friends and relatives should get it even if they are not asian i liked film's especially clever use of graphics as well as the smooth to score the and locations the perfect and pretty much everything else one of my favorite lines was about having state but you must to watch it for yourself i honestly have not laughed out loud during a film this much in decades oh and i rather liked chris too a totally hot actor even though i'm old enough to be his mother as soon as crazy rich opens i'm going to catch it again the preview was not enough and there were so many little moments that deserve a second or third look now we must all hope that a sequel with the same talented cast and in charge is coming our way before too long thank you all you fabulous asian actors crew writers and who made this possible and no i'm not of even a little bit asian\n",
      "as index (top 5000 only)\n",
      " [48, 3, 354, 17, 11, 13, 3, 52, 354, 19, 10, 1580, 2356, 8, 1, 435, 11, 17, 1012, 54, 689, 10, 78, 21, 383, 11, 17, 85, 42, 3, 434, 4, 55]\n",
      "as words (top 5000 only)\n",
      " what a boring movie this was a very boring film i fell asleep in the cinema this movie deserves no attention i do not recommend this movie because it's a waste of time\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "# Source: https://www.imdb.com/title/tt3104988/reviews\n",
    "\n",
    "test1 = \"\"\"I was so excited when I first learned that Kevin Kwan's \"Crazy Rich Asians\" was going to become a film! The book was way more appealing than I had first imagined it would be, and I'm happy to report that Jon Chu's screen version has surpassed my hopeful-but-wary expectations. Not to make it sound too simplistic, the movie was beautiful and very, very funny. Go see it!\n",
    "Yes, it is a romantic comedy - but this has such intriguing social and cultural undercurrents that it tempts even the fairly observant watcher away from taking the \"Cinderella\" story at its glitzy face value. While the numerous characters had to have their backstories compressed to fit into just two hours, we are given enough great dialogue, effervescent or slightly evil portrayals, and sumptuous visual clues to make the friends and family members in Singapore come alive.\n",
    "All the aunties, cousins and ladies-in-waiting may be slightly overwhelming for people who haven't read the book, but anyone with wacky friends and pompous relatives should get it, even if they are not Asian. \n",
    "I liked film's especially clever use of graphics, as well as the smooth-to-rocking score, the lush and verdant locations, the perfect designer costuming, and pretty much everything else. One of my favorite lines was about having attended Cal State Fullerton; but you must to watch it for yourself. I honestly have not laughed out loud during a film this much in decades. Oh, and I rather liked Chris Pang, too. A totally hot actor, even though I'm old enough to be his mother.\n",
    "As soon as Crazy Rich Asians officially opens, I'm going to catch it again. The preview was not enough, and there were so many little moments that deserve a second or third look. Now we must all hope that a sequel with the same talented cast and Chu in charge is coming our way before too long. Thank you all, you fabulous Asian actors, crew, writers and backers who made this possible. And no, I'm not of even a little bit Asian ancestry.\n",
    "\"\"\"\n",
    "test2 = \"\"\"what a boring movie. This was a very boring film. I fell asleep in the cinema. This movie deserves no attention! I do not recommend this movie because it's a waste of time.\"\"\"\n",
    "\n",
    "def clean_and_get_sequence(text):\n",
    "    # https://keras.io/preprocessing/text/#text_to_word_sequence\n",
    "    from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "    test_sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\n   ',\n",
    "                                          lower=True, split=' ')\n",
    "    # print('before cleaning:', test_sequence)\n",
    "\n",
    "    # drop words not in vocab\n",
    "    test_sequence_cleaned = [s for s in test_sequence if s in text_to_index]\n",
    "    #print('after cleaning:', len(test_sequence_cleaned))\n",
    "\n",
    "    # words that got dropped\n",
    "    #print('dropped words:', set(test_sequence) - set(test_sequence_cleaned))\n",
    "\n",
    "    # map to indices\n",
    "    test_sequence_index = [text_to_index[s] for s in test_sequence_cleaned]\n",
    "    #print('as index\\n', sequence_index)\n",
    "\n",
    "    # filter out top 5000\n",
    "    test_sequence_index_5000 = [i for i in test_sequence_index if i <= 5000]\n",
    "    print('as index (top 5000 only)\\n', test_sequence_index_5000)\n",
    "\n",
    "    # look at review\n",
    "    test_review = ' '.join([index_to_text[i] for i in test_sequence_index_5000])\n",
    "    print('as words (top 5000 only)\\n', test_review)\n",
    "    \n",
    "    return test_sequence_index_5000\n",
    "\n",
    "test1_index = clean_and_get_sequence(test1)\n",
    "test2_index = clean_and_get_sequence(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "test_reviews shape: (2, 400)\n"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing import sequence\n",
    "\n",
    "test_reviews = [test1_index, test2_index]\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "test_reviews = sequence.pad_sequences(test_reviews, maxlen=input_length)\n",
    "print('test_reviews shape:', test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was so excited when I first learned that Kevin Kwan's \"Crazy Rich Asians\" was going to become a film! The book was way more appealing than I had first imagined it would be, and I'm happy to report that Jon Chu's screen version has surpassed my hopeful-but-wary expectations. Not to make it sound too simplistic, the movie was beautiful and very, very funny. Go see it!\n",
      "Yes, it is a romantic comedy - but this has such intriguing social and cultural undercurrents that it tempts even the fairly observant watcher away from taking the \"Cinderella\" story at its glitzy face value. While the numerous characters had to have their backstories compressed to fit into just two hours, we are given enough great dialogue, effervescent or slightly evil portrayals, and sumptuous visual clues to make the friends and family members in Singapore come alive.\n",
      "All the aunties, cousins and ladies-in-waiting may be slightly overwhelming for people who haven't read the book, but anyone with wacky friends and pompous relatives should get it, even if they are not Asian. \n",
      "I liked film's especially clever use of graphics, as well as the smooth-to-rocking score, the lush and verdant locations, the perfect designer costuming, and pretty much everything else. One of my favorite lines was about having attended Cal State Fullerton; but you must to watch it for yourself. I honestly have not laughed out loud during a film this much in decades. Oh, and I rather liked Chris Pang, too. A totally hot actor, even though I'm old enough to be his mother.\n",
      "As soon as Crazy Rich Asians officially opens, I'm going to catch it again. The preview was not enough, and there were so many little moments that deserve a second or third look. Now we must all hope that a sequel with the same talented cast and Chu in charge is coming our way before too long. Thank you all, you fabulous Asian actors, crew, writers and backers who made this possible. And no, I'm not of even a little bit Asian ancestry.\n",
      "\n",
      "positive probability [0.9651406]\n",
      "-------\n",
      "what a boring movie. This was a very boring film. I fell asleep in the cinema. This movie deserves no attention! I do not recommend this movie because it's a waste of time.\n",
      "positive probability [0.91152096]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "tests = [test1, test2]\n",
    "pred_prob = sentiment_decoder.predict(test_reviews)\n",
    "# pred_label = sentiment_decoder.predict_classes(test_reviews)\n",
    "\n",
    "for text, probability in zip(tests, pred_prob):\n",
    "    print(text)\n",
    "    print('positive', 'probability', probability)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
